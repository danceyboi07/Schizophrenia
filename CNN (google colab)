def get_paths_and_labels(csv, source_path):

    binary_class = []
    filepaths = []
    for key, value in csv.iterrows():
        binary_class.append(value[1])
        subject_path = source_path + 'fmri_' + value[0][:-1] + '_session1_run1'
        fmri_path = subject_path + '.nii.gz'
        filepaths.append(fmri_path)
    return filepaths, binary_class

!pip install nilearn

from nilearn.connectome import ConnectivityMeasure
from nilearn import decomposition, image, plotting, input_data
import csv


def group_ica(fmri_files):
    ica = decomposition.CanICA(n_components=20, mask_strategy='whole-brain-template')
    print('decomposition done')
    ica.fit(fmri_files)
    print('fitting done')
    components = ica.components_
    components_inverse = ica.masker_.inverse_transform(components)
    # plotting.plot_stat_map(image.index_img(components_inverse, 10))
    # plotting.show()

    # plotting.plot_prob_atlas(components_inverse)
    # plotting.show()

    masker = input_data.NiftiMapsMasker(components_inverse,
                                        smoothing_fwhm=6, detrend=True,
                                        t_r=2.5, low_pass=0.1, high_pass=0.01)
    time_series_subjects = []
    i = 1
    for f in fmri_files:
        print(i, 'Currently processing:', f)
        i += 1
        time_series = masker.fit_transform(f)
        time_series_subjects.append(time_series)

    print('time_series_subjects array filled')
    correlation_measure = ConnectivityMeasure(kind='correlation')
    corr_mats = correlation_measure.fit_transform(time_series_subjects)

    # coords = plotting.find_probabilistic_atlas_cut_coords(components_inverse)
    # plotting.plot_connectome(corr_mats[0], coords, edge_threshold="80%")
    # plotting.show()

    return corr_mats


def write_to_csv(matrices, csv_path):
    print("The datatype of the matrices we're workikng with:")
    print(type(matrices[0]))
    with open(csv_path, 'w') as f:
        for m in matrices:
            one_dim = m.reshape(1, -1).tolist()
            writer = csv.writer(f)
            writer.writerow(one_dim)

import pandas as pd

from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import csv
import numpy as np
from datetime import datetime
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split
#from data_prep import get_paths_and_labels

torch.manual_seed(1)


class CobreDataset(Dataset):
    def __init__(self, data, labels):
        self.x = torch.from_numpy(data)
        self.y = torch.from_numpy(labels)
        self.n_samples = labels.shape[0]
        self.targets = labels

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return self.n_samples


class NeuralNetwork(nn.Module):

    def __init__(self, input_shape):
        super().__init__()
        self.lin1 = nn.Linear(input_shape, 64)
        self.dropout1 = nn.Dropout(0.4)
        self.lin2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.4)
        self.lin4 = nn.Linear(32, 1)

    def forward(self, x):
        x = torch.tanh(self.lin1(x))
        x = self.dropout1(x)
        x = torch.relu(self.lin2(x))
        x = self.dropout2(x)
        x = torch.sigmoid(self.lin4(x))
        return x


def save_model(model):
    now = datetime.now()
    time_str = now.strftime("%d_%m_%Y_%H%M%S")
    path = "./model_" + time_str + ".pth"
    torch.save(model.state_dict(), path)


def training():
    losses = []
    val_losses = []
    accuracies = []
    val_acc = []
    for epoch in range(num_epochs):
        print('EPOCH:', epoch + 1)
        model.train(True)
        running_loss = 0.0
        len_data = 0
        correct = 0
        for i, (inputs, labels) in enumerate(train_batches):

            outputs = model(inputs.cuda())
            loss = loss_f(outputs, labels.cuda().unsqueeze(1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            for o, l in zip(outputs, labels):
                if o >= 0.5:
                    b_o = 1.0
                else:
                    b_o = 0.0
                if b_o == l:
                    correct += 1
            running_loss += loss.item()
            len_data += batch_size
        accuracy = correct / len_data
        avg_loss = running_loss / len_data
        print(avg_loss)
        print("Accuracy:", accuracy)
        accuracies.append(accuracy)
        losses.append(avg_loss)
        model.train(False)


        valid_loss = 0.0
        len_val_data = 0
        correct = 0
        for i, (inputs, labels) in enumerate(val_batches):
            outputs = model(inputs.cuda())
            loss = loss_f(outputs, labels.cuda().unsqueeze(1))
            optimizer.zero_grad()
            for o, l in zip(outputs, labels):
                if o >= 0.5:
                    b_o = 1.0
                else:
                    b_o = 0.0
                if b_o == l:
                    correct += 1
            valid_loss += loss.item()
            len_val_data += batch_size
        accuracy = correct / len_data
        print(valid_loss / len_val_data)
        print("Val accuracy:", accuracy)
        val_acc.append(accuracy)
        val_losses.append(valid_loss / len_val_data)

    save_model(model)
    plt.plot(losses, label='train loss')
    plt.plot(val_losses, label='validation loss')
    plt.legend()
    plt.show()

    plt.plot(accuracies, label='train accuracy')
    plt.plot(val_acc, label='validation accuracy')
    plt.legend()
    plt.show()
    return model


if __name__ == '__main__':

    source_path = '/content/drive/MyDrive/COBRE_fmri/'
    csv_path = pd.read_csv(source_path + 'cobre_model_group.csv')
    filepaths, binary_class = get_paths_and_labels(csv_path, source_path)
    csvFC_path = '/content/drive/MyDrive/Colab Notebooks/nn_FCs.csv'
    corr_mats_in_line = []
    with open(csvFC_path, 'r') as f:
        csv_reader = csv.reader(f, delimiter=',')
        for row in csv_reader:
            corr_mat = np.array(row[0].strip('][').split(', ')).reshape(20, -1)
            corr_mats_in_line.append(np.array(row[0].strip('][').split(', ')).astype(np.float32))
    dataset = CobreDataset(np.array(corr_mats_in_line, dtype=np.float32), np.array(binary_class, dtype=np.float32))
    train_indices, test_indices, _, _ = train_test_split(range(len(dataset)),
                                                         dataset.targets,
                                                         stratify=dataset.targets,
                                                         test_size=0.2,
                                                         random_state=16)

    batch_size = 5
    num_epochs = 30

    #train_split, val_split, test_split = random_split(dataset, (0.65, 0.15, 0.2))
    proportions = [.65, .15, .2]
    lengths = [int(p * len(dataset)) for p in proportions]
    lengths[-1] = len(dataset) - sum(lengths[:-1])
    train_split, val_split, test_split = random_split(dataset, lengths)
    train_batches = DataLoader(dataset=train_split, batch_size=batch_size, shuffle=True)
    val_batches = DataLoader(dataset=val_split, batch_size=batch_size)
    test_batches = DataLoader(dataset=test_split, batch_size=batch_size)

    device = torch.device('cpu')
    if torch.cuda.is_available():
        print('Cuda is available')
        device = torch.device('cuda')

    model = NeuralNetwork(input_shape=400)
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_f = nn.BCELoss()

    fetch_saved = True  # put True for testing a model saved in the path variable, False for training and testing a new model

    if fetch_saved:
        path = "/content/drive/MyDrive/Colab Notebooks/NN_model_from_paper.pth"
        print("Testing with model", path)
        model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))
        model.to(device)
    else:
        training()

    with torch.no_grad():
        correct_hc, correct_sz, false_hc, false_sz = 0, 0, 0, 0
        running_accuracy = 0
        total = 0

        y_true = []
        y_pred = []

        for i, (inputs, labels) in enumerate(test_batches):
            predicted = model(inputs.cpu())
            for p, lab in zip(predicted, labels):
                if p.item() >= 0.5:
                    binary_pred = 1.0
                else:
                    binary_pred = 0.0

                y_true.append(lab.item())
                y_pred.append(binary_pred)

                if lab == binary_pred:
                    running_accuracy += 1
                    if binary_pred == 0.0:
                        correct_hc += 1
                    else:
                        correct_sz += 1
                else:
                    if binary_pred == 0.0:
                        false_hc += 1
                    else:
                        false_sz += 1
                total += 1
    print("Accuracy:", running_accuracy/total)
    print("Correct HC:", correct_hc)
    print("Correct SZ:", correct_sz)
    print("False HC:", false_hc)
    print("False SZ:", false_sz)
    print()

    cm = confusion_matrix(y_true, y_pred, labels=[0.0, 1.0])
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy Control (HC)', 'Schizophrenia (SZ)'])
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()
